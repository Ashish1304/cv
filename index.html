<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ashish George</title>
</head>

<body>
    <img src="img/ashish.png" alt="" width="250" height="250">
    <h1>Ashish George</h1>
    <p>
        <em>Data Architect and Engineer </em>
    </p>
    <p>
        Skilled Data Solution Specialist with over 11 years of IT experience in customer-facing roles. 
        <br>
        Proficient as a full-stack developer, having led multiple Hadoop Development Teams across Insurance, Medical
        Devices, and Marketing sectors. 
        <br>
        Expertise in data migration, solution design, and implementation on both on-premises and cloud infrastructure. Ability to learn new technologies quickly.
        <br>
    </p>
    <hr size="3" noshade="">

    <p>
        <h3>Summary of Qualifications</h3>

        <h4>Proficient in:</h4>
        <ul>
            <li>Data Engineering
                <ul>
                    <li>Snowflake, Cloud Data Solution Design, Adverity, Airflow, Dagster, SQL, pyspark</li>
                    <li>Hadoop, Impala, Hive, SAP HANA</li>
                </ul>    
            </li>
            <li>Application Development
                <ul>
                    <li>Python, Javascript, HTML, CSS</li>
                </ul>    
            </li>
            <li>Agile and Kanban Methodologies</li>
            <li>Offshore-Onsite Team Management</li>
        </ul>

        <h4>Languages</h4>
        <ul>
            <li>English (Fluent)</li>
            <li>Spanish (beginner)</li>
        </ul>

    </p>
    <hr>
    <p>
        <h3>Career History</h3>
        <h4><b>Baires Dev </b>, Jan 2022 -current, Senior Data Engineer</h4>
        <ul>
             <li> Designed and managed role based data access for users in Snowflake.</li>
             <li> Data Warehouse management, User-Role Management in Snowflake </li>
             <li> Worked with Airflow ETL jobs for collecting data from various source APIs using pySpark and
                  AWS S3 and loading into the data warehouse. </li>
             <li> Designed and implemented automation on Streamlit application for data loading via Adverity</li>
             <li> Worked on SOX views identification and tagging in Snowflake</li>
             <li> Created user access monitoring calculation views in SAP HANA.</li>
        </ul>

        <br>
        <h4><b>Globant, Guadalajara, Mexico </b>, Jan 2020 - Jan 2022, Data Architect</h4>
        <ul>
            <li>Created and maintained ETLs on Airflow</li>
            <li>Developed ETL solutions primarily using Python and Snowflake.</li>
            <li>Helped client to migrate from on-premise system to cloud by designing and implementing data
solution on major cloud vendors :
                <ul>
                    <li>AWS (S3, Glue, Kinesis, Redshift, Athena)</li>
                    <li>GCP (GCS, Pub/Sub, Big Query)</li>
                    <li>Azure (ADLS, Event Hub, Synapse Analytics, ADF, DataBricks)</li>
                </ul>    
            </li>
        </ul>

        <br>
        <h4><b>Tata Consultancy Services, Guadalajara, Mexico </b>, May 2018 - Jan 2020 Hadoop Technical Lead</h4>
        <ul>
            <li>Collaborated with customers to understand the data ingestion and reporting requirements in Hadoop
                Enterprise Data Lake</li>
            <li>Spearheaded offshore teams, for the design, development and testing of scripts and queries following
                agile life-cycle model</li>
            <li>Implemented data ingestion from files and RDBMS systems and table creation with Hive queries based on
                business requirements, to be accessed via QlikView and Tableau by the customers.</li>
            <li>Received customer appreciation for pro-activeness and timely delivery</li>
        </ul>

        <br>
        <h4><b>Tata Consultancy Services, Stockholm, Sweden </b>, July 2016 - Dec 2016 Hadoop Technical Lead</h4>
        <ul>
            <li>Successfully implemented TCS Big Data Product (Active Archiveâ„¢) for a customer in the insurance industry.</li>
            <li>Provided solution for the management of millions of small size files</li>
            <li>Received customer appreciation for problems solving and timely delivery</li>
        </ul>

        <br>
        <h4><b>Tata Consultancy Services, Cochin, India </b>, May 2014 - April 2018 Java and Hadoop Developer</h4>
        <ul>
            <li>Worked as a full stack developer for the TCS Big Data Product, Active ArchiveTM</li>
            <li>Worked on UI design, web-services and Java code for interaction with Hadoop platform</li>
            <li>Implemented fuzzy website content search in <a href="https://www.tcs.com/" target="_blank">TCS
                    Website</a> using solr </li>
        </ul>

        <br>
        <h4><b>Tata Consultancy Services, Trivandrum, India </b>, July 2013 - April 2014 J2EE Trainer</h4>
        <ul>
            <li>Provided J2EE training to new employees</li>
            <li>Collaborated with the Learning and Development department to develop an effective training program</li>
        </ul>
        <br>
    </p>
    <p>
        <h3>Education</h3>

        <a href="https://www.srmist.edu.in/" target="_blank">SRM University</a> , Chennai, India Aug. 2009 - May 2013 Bachelors in Technology, Computer Science
    </p>

</body>

</html>
